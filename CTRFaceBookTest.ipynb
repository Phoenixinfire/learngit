{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/neal668/LightGBM-GBDT-LR/blob/master/GBFT%2BLR_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genderDataShape:(72543, 139)\n"
     ]
    }
   ],
   "source": [
    "genderData=pd.read_csv('./GenderFeatureData.data',header=None).drop(0,axis=1)\n",
    "print(\"genderDataShape:{}\".format(genderData.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=genderData[1]\n",
    "X_train=genderData.drop(1,axis=1)\n",
    "lgb_train=lgb.Dataset(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:163: UserWarning: Using Pandas (default) integer column names, not column indexes. You can use indexes with DataFrame.values.\n",
      "  booster = Booster(params=params, train_set=train_set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 0.687967\n",
      "[2]\ttraining's binary_logloss: 0.682982\n",
      "[3]\ttraining's binary_logloss: 0.678392\n",
      "[4]\ttraining's binary_logloss: 0.673596\n",
      "[5]\ttraining's binary_logloss: 0.668813\n",
      "[6]\ttraining's binary_logloss: 0.664193\n",
      "[7]\ttraining's binary_logloss: 0.659775\n",
      "[8]\ttraining's binary_logloss: 0.655439\n",
      "[9]\ttraining's binary_logloss: 0.650965\n",
      "[10]\ttraining's binary_logloss: 0.64657\n",
      "[11]\ttraining's binary_logloss: 0.642288\n",
      "[12]\ttraining's binary_logloss: 0.638052\n",
      "[13]\ttraining's binary_logloss: 0.633931\n",
      "[14]\ttraining's binary_logloss: 0.630252\n",
      "[15]\ttraining's binary_logloss: 0.626513\n",
      "[16]\ttraining's binary_logloss: 0.622993\n",
      "[17]\ttraining's binary_logloss: 0.61909\n",
      "[18]\ttraining's binary_logloss: 0.61545\n",
      "[19]\ttraining's binary_logloss: 0.611741\n",
      "[20]\ttraining's binary_logloss: 0.608847\n",
      "[21]\ttraining's binary_logloss: 0.605509\n",
      "[22]\ttraining's binary_logloss: 0.601897\n",
      "[23]\ttraining's binary_logloss: 0.598849\n",
      "[24]\ttraining's binary_logloss: 0.595594\n",
      "[25]\ttraining's binary_logloss: 0.592474\n",
      "[26]\ttraining's binary_logloss: 0.589079\n",
      "[27]\ttraining's binary_logloss: 0.585742\n",
      "[28]\ttraining's binary_logloss: 0.583004\n",
      "[29]\ttraining's binary_logloss: 0.579837\n",
      "[30]\ttraining's binary_logloss: 0.576694\n",
      "[31]\ttraining's binary_logloss: 0.573567\n",
      "[32]\ttraining's binary_logloss: 0.570492\n",
      "[33]\ttraining's binary_logloss: 0.567753\n",
      "[34]\ttraining's binary_logloss: 0.564783\n",
      "[35]\ttraining's binary_logloss: 0.56185\n",
      "[36]\ttraining's binary_logloss: 0.558982\n",
      "[37]\ttraining's binary_logloss: 0.556159\n",
      "[38]\ttraining's binary_logloss: 0.55359\n",
      "[39]\ttraining's binary_logloss: 0.550829\n",
      "[40]\ttraining's binary_logloss: 0.54823\n",
      "[41]\ttraining's binary_logloss: 0.545778\n",
      "[42]\ttraining's binary_logloss: 0.543349\n",
      "[43]\ttraining's binary_logloss: 0.540771\n",
      "[44]\ttraining's binary_logloss: 0.538422\n",
      "[45]\ttraining's binary_logloss: 0.535909\n",
      "[46]\ttraining's binary_logloss: 0.53342\n",
      "[47]\ttraining's binary_logloss: 0.531049\n",
      "[48]\ttraining's binary_logloss: 0.528641\n",
      "[49]\ttraining's binary_logloss: 0.526383\n",
      "[50]\ttraining's binary_logloss: 0.524043\n",
      "[51]\ttraining's binary_logloss: 0.52193\n",
      "[52]\ttraining's binary_logloss: 0.519669\n",
      "[53]\ttraining's binary_logloss: 0.517423\n",
      "[54]\ttraining's binary_logloss: 0.515226\n",
      "[55]\ttraining's binary_logloss: 0.513055\n",
      "[56]\ttraining's binary_logloss: 0.51094\n",
      "[57]\ttraining's binary_logloss: 0.508828\n",
      "[58]\ttraining's binary_logloss: 0.506846\n",
      "[59]\ttraining's binary_logloss: 0.504784\n",
      "[60]\ttraining's binary_logloss: 0.502987\n",
      "[61]\ttraining's binary_logloss: 0.501103\n",
      "[62]\ttraining's binary_logloss: 0.499135\n",
      "[63]\ttraining's binary_logloss: 0.497196\n",
      "[64]\ttraining's binary_logloss: 0.495444\n",
      "[65]\ttraining's binary_logloss: 0.493553\n",
      "[66]\ttraining's binary_logloss: 0.491831\n",
      "[67]\ttraining's binary_logloss: 0.490028\n",
      "[68]\ttraining's binary_logloss: 0.488219\n",
      "[69]\ttraining's binary_logloss: 0.486419\n",
      "[70]\ttraining's binary_logloss: 0.484658\n",
      "[71]\ttraining's binary_logloss: 0.482986\n",
      "[72]\ttraining's binary_logloss: 0.481264\n",
      "[73]\ttraining's binary_logloss: 0.47981\n",
      "[74]\ttraining's binary_logloss: 0.47818\n",
      "[75]\ttraining's binary_logloss: 0.476547\n",
      "[76]\ttraining's binary_logloss: 0.474905\n",
      "[77]\ttraining's binary_logloss: 0.473437\n",
      "[78]\ttraining's binary_logloss: 0.471845\n",
      "[79]\ttraining's binary_logloss: 0.470341\n",
      "[80]\ttraining's binary_logloss: 0.468831\n",
      "[81]\ttraining's binary_logloss: 0.467308\n",
      "[82]\ttraining's binary_logloss: 0.465936\n",
      "[83]\ttraining's binary_logloss: 0.464442\n",
      "[84]\ttraining's binary_logloss: 0.462983\n",
      "[85]\ttraining's binary_logloss: 0.461711\n",
      "[86]\ttraining's binary_logloss: 0.460324\n",
      "[87]\ttraining's binary_logloss: 0.458916\n",
      "[88]\ttraining's binary_logloss: 0.458074\n",
      "[89]\ttraining's binary_logloss: 0.456687\n",
      "[90]\ttraining's binary_logloss: 0.455864\n",
      "[91]\ttraining's binary_logloss: 0.454498\n",
      "[92]\ttraining's binary_logloss: 0.453165\n",
      "[93]\ttraining's binary_logloss: 0.451957\n",
      "[94]\ttraining's binary_logloss: 0.450637\n",
      "[95]\ttraining's binary_logloss: 0.449457\n",
      "[96]\ttraining's binary_logloss: 0.448192\n",
      "[97]\ttraining's binary_logloss: 0.446925\n",
      "[98]\ttraining's binary_logloss: 0.445676\n",
      "[99]\ttraining's binary_logloss: 0.444938\n",
      "[100]\ttraining's binary_logloss: 0.443717\n"
     ]
    }
   ],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 63,\n",
    "    'num_trees': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# number of leaves,will be used in feature transformation\n",
    "num_leaf = 63\n",
    "\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2881: UserWarning: Using Pandas (default) integer column names, not column indexes. You can use indexes with DataFrame.values.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_train,pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72543\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(len(y_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[  13  104  187  200  304  327  427  499  517  580  685  706  770  875  907\n",
      "  979 1066 1126 1174 1241 1309 1368 1432 1509 1573 1588 1651 1752 1805 1883\n",
      " 1944 2005 2065 2132 2155 2252 2281 2377 2407 2492 2530 2593 2705 2748 2785\n",
      " 2848 2937 2998 3080 3142 3201 3256 3321 3381 3456 3517 3580 3647 3667 3771\n",
      " 3840 3900 3962 3979 4069 4106 4168 4257 4297 4360 4447 4528 4549 4646 4695\n",
      " 4757 4844 4864 4955 5003 5053 5160 5179 5280 5302 5414 5432 5493 5579 5619\n",
      " 5702 5745 5806 5872 5932 5995 6062 6125 6189 6283]\n",
      "100\n",
      "[  61  109  184  239  285  357  407  473  540  611  690  747  809  877  922\n",
      "  981 1069 1089 1195 1231 1301 1342 1440 1505 1555 1597 1660 1734 1823 1848\n",
      " 1948 2009 2029 2134 2202 2247 2324 2382 2439 2511 2572 2638 2692 2769 2832\n",
      " 2858 2958 3005 3070 3111 3205 3252 3297 3376 3440 3515 3579 3630 3692 3774\n",
      " 3815 3889 3952 4017 4085 4142 4215 4244 4339 4370 4441 4510 4577 4626 4713\n",
      " 4761 4836 4901 4926 5034 5074 5134 5224 5283 5352 5403 5459 5518 5597 5648\n",
      " 5691 5787 5847 5918 5973 6041 6099 6165 6212 6294]\n",
      "100\n",
      "[  29  109  180  217  280  355  436  502  550  615  678  740  785  849  911\n",
      "  968 1054 1113 1177 1236 1308 1374 1440 1505 1558 1615 1678 1742 1793 1861\n",
      " 1950 2011 2051 2139 2200 2245 2302 2363 2430 2512 2545 2609 2675 2727 2801\n",
      " 2873 2942 2984 3059 3137 3210 3271 3336 3376 3440 3515 3579 3611 3706 3740\n",
      " 3831 3893 3956 3999 4088 4119 4213 4276 4317 4403 4471 4482 4572 4631 4701\n",
      " 4767 4837 4894 4918 5028 5092 5158 5174 5276 5353 5413 5462 5542 5573 5667\n",
      " 5722 5740 5803 5867 5975 6015 6106 6170 6231 6295]\n",
      "100\n",
      "[   0   63  126  189  252  315  378  441  504  567  630  693  756  864  882\n",
      "  945 1008 1071 1134 1197 1260 1323 1386 1449 1512 1608 1671 1716 1792 1856\n",
      " 1890 1953 2076 2079 2142 2205 2268 2374 2394 2457 2561 2627 2646 2709 2772\n",
      " 2835 2898 2961 3024 3087 3186 3245 3308 3370 3431 3501 3564 3623 3690 3717\n",
      " 3797 3863 3926 3986 4051 4123 4189 4255 4316 4379 4430 4492 4554 4617 4682\n",
      " 4765 4788 4892 4943 5009 5040 5128 5166 5229 5330 5355 5418 5535 5544 5620\n",
      " 5670 5733 5820 5859 5947 5999 6064 6127 6217 6254]\n",
      "100\n",
      "[  21   81  153  207  272  337  395  457  527  590  649  714  778  836  905\n",
      "  987 1030 1100 1154 1245 1286 1346 1408 1467 1526 1604 1667 1723 1789 1852\n",
      " 1910 1973 2032 2098 2161 2223 2290 2348 2413 2474 2539 2601 2664 2738 2791\n",
      " 2849 2910 2976 3038 3101 3206 3226 3290 3353 3416 3488 3552 3605 3679 3769\n",
      " 3794 3856 3919 4010 4046 4110 4172 4235 4298 4361 4425 4494 4535 4658 4683\n",
      " 4739 4792 4865 4933 4989 5054 5115 5180 5242 5327 5365 5431 5508 5569 5635\n",
      " 5684 5746 5811 5873 5935 6039 6061 6123 6205 6252]\n"
     ]
    }
   ],
   "source": [
    "transformed_training_matrix = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "transformed_training_matrix.shape\n",
    "for i in range(0,len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "    transformed_training_matrix[i][temp] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -1,   62,  125,  188,  251,  314,  377,  440,  503,  566,  629,\n",
       "        692,  755,  818,  881,  944, 1007, 1070, 1133, 1196, 1259, 1322,\n",
       "       1385, 1448, 1511, 1574, 1637, 1700, 1763, 1826, 1889, 1952, 2015,\n",
       "       2078, 2141, 2204, 2267, 2330, 2393, 2456, 2519, 2582, 2645, 2708,\n",
       "       2771, 2834, 2897, 2960, 3023, 3086, 3149, 3212, 3275, 3338, 3401,\n",
       "       3464, 3527, 3590, 3653, 3716, 3779, 3842, 3905, 3968, 4031, 4094,\n",
       "       4157, 4220, 4283, 4346, 4409, 4472, 4535, 4598, 4661, 4724, 4787,\n",
       "       4850, 4913, 4976, 5039, 5102, 5165, 5228, 5291, 5354, 5417, 5480,\n",
       "       5543, 5606, 5669, 5732, 5795, 5858, 5921, 5984, 6047, 6110, 6173,\n",
       "       6236])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(y_pred[0])) * num_leaf-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Writing transformed testing data')\n",
    "transformed_testing_matrix = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "for i in range(0,len(y_pred)):\n",
    "\ttemp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "\ttransformed_testing_matrix[i][temp] += 1\n",
    "\n",
    "#for i in range(0,len(y_pred)):\n",
    "#\tfor j in range(0,len(y_pred[i])):\n",
    "#\t\ttransformed_testing_matrix[i][j * num_leaf + y_pred[i][j]-1] = 1\n",
    "\n",
    "print('Calculate feature importances...')\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))\n",
    "print('Feature importances:', list(gbm.feature_importance(\"gain\")))\n",
    "\n",
    "# Logestic Regression Start\n",
    "print(\"Logestic Regression Start\")\n",
    "\n",
    "# load or create your dataset\n",
    "print('Load data...')\n",
    "\n",
    "c = np.array([1,0.5,0.1,0.05,0.01,0.005,0.001])\n",
    "for t in range(0,len(c)):\n",
    "\tlm = LogisticRegression(penalty='l2',C=c[t]) # logestic model construction\n",
    "\tlm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
    "\n",
    "\t#y_pred_label = lm.predict(transformed_training_matrix )  # For training data\n",
    "\t#y_pred_label = lm.predict(transformed_testing_matrix)    # For testing data\n",
    "\t#y_pred_est = lm.predict_proba(transformed_training_matrix)   # Give the probabilty on each label\n",
    "\ty_pred_est = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n",
    "\n",
    "#print('number of testing data is ' + str(len(y_pred_label)))\n",
    "#print(y_pred_est)\n",
    "\n",
    "# calculate predict accuracy\n",
    "\t#num = 0\n",
    "\t#for i in range(0,len(y_pred_label)):\n",
    "\t\t#if y_test[i] == y_pred_label[i]:\n",
    "\t#\tif y_train[i] == y_pred_label[i]:\n",
    "\t#\t\tnum += 1\n",
    "\t#print('penalty parameter is '+ str(c[t]))\n",
    "\t#print(\"prediction accuracy is \" + str((num)/len(y_pred_label)))\n",
    "\n",
    "\t# Calculate the Normalized Cross-Entropy\n",
    "\t# for testing data\n",
    "\tNE = (-1) / len(y_pred_est) * sum(((1+y_test)/2 * np.log(y_pred_est[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_est[:,1])))\n",
    "\t# for training data\n",
    "\t#NE = (-1) / len(y_pred_est) * sum(((1+y_train)/2 * np.log(y_pred_est[:,1]) +  (1-y_train)/2 * np.log(1 - y_pred_est[:,1])))\n",
    "\tprint(\"Normalized Cross Entropy \" + str(NE))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
